---
title: "ANN ajusted to bidimensional data"
subtitle: "A visual example of how to recalibrate a neural network"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ANN ajusted to bidimensional data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = F, 
  comment = "#>"
)
```

```{r setup}
library(recalibratiNN)
```

```{r}
set.seed(42)
n <- 10000

x <- cbind(x1 = runif(n, -3, 3),
           x2 = runif(n, -5, 5))

mu_fun <- function(x) {
  abs(x[,1]^3 - 50*sin(x[,2]) + 30)}

mu <- mu_fun(x)
y <- rnorm(n, 
           mean = mu, 
           sd=20*(abs(x[,2]/(x[,1]+ 10))))

split1 <- 0.6
split2 <- 0.8

x_train <- x[1:(split1*n),]
y_train <- y[1:(split1*n)]

x_cal  <- x[(split1*n+1):(n*split2),]
y_cal  <- y[(split1*n+1):(n*split2)]

x_test <- x[(split2*n+1):n,]
y_test  <- y[(split2*n+1):n]

```


```{r, eval=F}
model_nn <- keras_model_sequential()

model_nn |> 
  layer_dense(input_shape=2,
              units=800,
              use_bias=T,
              activation = "relu",
              kernel_initializer="random_normal",
              bias_initializer = "zeros") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units=800,
              use_bias=T,
              activation = "relu",
              kernel_initializer="random_normal",
              bias_initializer = "zeros") |> 
  layer_dropout(rate = 0.1) |> 
  layer_dense(units=800,
              use_bias=T,
              activation = "relu",
              kernel_initializer="random_normal",
              bias_initializer = "zeros") |> 
   layer_batch_normalization() |> 
  layer_dense(units = 1,
              activation = "linear",
              kernel_initializer = "zeros",
              bias_initializer = "zeros")

model_nn |> 
  compile(optimizer=optimizer_adam( ),
    loss = "mse")

model_nn |> 
  fit(x = x_train, 
      y = y_train,
      validation_data = list(x_cal, y_cal),
      callbacks = callback_early_stopping(
        monitor = "val_loss",
        patience = 20,
        restore_best_weights = T),
      batch_size = 128,
      epochs = 1000)


y_hat_cal <- predict(model_nn, x_cal)
y_hat_test <- predict(model_nn, x_test)
```


```{r}
file_path1 <- system.file("extdata", "mse_cal.txt", package = "recalibratiNN")
read.table(file_path1) |> as.numeric()
```


```{r, eval = T}
file_path1 <- system.file("extdata", "mse_cal.rds", package = "recalibratiNN")
readRDS(file_path1)



```

```{r, eval=F}
# carregar os vetores .rds
file_path2 <- system.file("extdata", "y_hat_cal.rds", package = "recalibratiNN")
y_hat_cal <- readRDS(file_path2)

y_hat_test <- file_path3 <- system.file("extdata", "y_hat_test.rds", package = "recalibratiNN")
y_hat_test <- readRDS(file_path3)
y_hat_cal <- readRDS("y_hat_cal.rds") |> as.numeric()
y_hat_test <- readRDS("y_hat_test.rds")|> as.numeric()
MSE_cal <- readRDS("MSE_cal.rds")|> as.numeric()

```



```{r}

## Global calibrations
pit <- PIT_global(ycal = y_cal, 
                  yhat = y_hat_cal, 
                  mse = MSE_cal)

gg_PIT_global(pit)

```

```{r}
pit_local <- PIT_local(xcal = x_cal,
                       ycal = y_cal, 
                       yhat = y_hat_cal, 
                       mse = MSE_cal
                       )

gg_PIT_local(pit_local, 
             facet = TRUE)

```

```{r}
coverage_model <- tidyr::tibble(
  x1cal = x_test[,1], 
  x2cal = x_test[,2],
  y_real = y_test, 
  y_hat = y_hat_test) |> 
dplyr::mutate(lwr = qnorm(0.05, y_hat, sqrt(MSE_cal)),
       upr = qnorm(0.95, y_hat, sqrt(MSE_cal)),
       CI = ifelse(y_real <= upr & y_real >= lwr, 
                       "in",  "out" ),
       coverage = round(mean(CI == "in")*100,1) 
)

coverage_model |> 
  ggplot2::ggplot() +
  ggplot2::geom_point(ggplot2::aes(x1cal, 
                 x2cal, 
                 color = CI),
             alpha = 0.8)+
   ggplot2::labs(x="x1" , y="x2", 
        title = glue::glue("Original coverage: {coverage_model$coverage[1]} %"))+
  ggplot2::scale_color_manual("Confidence Interval",
                     values = c("in" = "aquamarine3", 
                                "out" = "steelblue4"))+
  ggplot2::theme_classic()
```

```{r}
recalibrated <- 
  recalibrate(
    yhat_new = y_hat_test,
    space_cal = x_cal,
    pit_values = pit,
    space_new = x_test,
    mse = MSE_cal, 
    type="local",  
    p_neighbours = 0.08)

y_hat_rec <- recalibrated$y_samples_calibrated_wt
```

```{r}
 coverage_rec <- purrr::map_dfr( 1:nrow(x_test), ~ {
  quantile(y_hat_rec[.,]
           ,c(0.05, 0.95))}) |> 
  dplyr::mutate(
    x1 = x_test[,1],
    x2 = x_test[,2],
    ytest = y_test,
    CI = ifelse(ytest <= `95%`& ytest >= `5%`, 
                "in", "out"),
    coverage = round(mean(CI == "in")*100,1))

 coverage_rec |> 
   ggplot2::ggplot() +
   ggplot2::geom_point(ggplot2::aes(x1, x2, color = CI),
              alpha = 0.7)+
   ggplot2::labs(x="x1" , y="x2", 
        title = glue::glue("Recalibrated coverage: {coverage_rec$coverage[1]} %"))+
  ggplot2::scale_color_manual("Confidence Interval",
                     values = c("in" = "aquamarine3", 
                                "out" = "steelblue4"))+
  ggplot2::theme_classic()
```
```{r}

n_clusters <- 6 
n_neighbours <- length(y_hat_test)*0.08


# calculating centroids
cluster_means_cal <- kmeans(x_test, n_clusters)$centers

cluster_means_cal <- cluster_means_cal[order(cluster_means_cal[,1]),]

  
# finding neighbours
knn_cal <- RANN::nn2(x_test, 
               cluster_means_cal, 
               k = n_neighbours)$nn.idx


# geting corresponding ys (real and estimated)
y_real_local <- purrr::map(1:nrow(knn_cal),  ~y_test[knn_cal[.,]])

y_hat_local <- purrr::map(1:nrow(knn_cal),  ~y_hat_rec[knn_cal[.,],])


# calculate pit_local
pits <- matrix(NA, 
               nrow = 6, 
               ncol = n_neighbours)

for (i in 1:n_clusters) {
    pits[i,] <- purrr::map_dbl(1:n_neighbours, ~{
      mean(y_hat_local[[i]][.,] <= y_hat_local[[i]][.])
    })
}

as.data.frame(t(pits)) |> 
  tidyr::pivot_longer(everything()) |> 
  dplyr::group_by(name) |>
  dplyr::mutate(p_value =ks.test(value,
                          "punif")$p.value,
         name = gsub("V", "part_", name)) |> 
  ggplot2::ggplot()+
  ggplot2::geom_density(ggplot2::aes(value,
                   color = name,
                   fill = name),
               alpha = 0.5,
               bounds = c(0, 1))+
  ggplot2::geom_hline(yintercept = 1, 
             linetype="dashed")+
  ggplot2::scale_color_brewer(palette = "Set2")+
  ggplot2::scale_fill_brewer(palette = "Set2")+
  ggplot2::theme_classic()+
  ggplot2::geom_text(ggplot2::aes(x = 0.5, 
                y = 0.5,
                label = glue::glue("p-value: {round(p_value, 3)}")),
            color = "black",
            size = 3)+
  ggplot2::theme(legend.position = "none")+
  ggplot2::labs(title = "After Local Calibration",
       subtitle = "It looks so much better!!",
       x = "PIT-values",
       y = "Density")+
  ggplot2::facet_wrap(~name, scales = "free_y")
```

```{r}
data.frame(
  real = mu_fun(x_test),
  desc = y_hat_test,
  recal = recalibrated$y_hat_calibrated
) |> 
  tidyr::pivot_longer(-real) |> 
  ggplot2::ggplot()+
  ggplot2::geom_point(ggplot2::aes( x = value,
                  y = real,
                  color = name),
             alpha = 0.8)+ 
  ggplot2::scale_color_manual("", values = c( "#003366","#80b298"),
                     labels = c("Predicted", "Recalibrated"))+
  ggplot2::geom_abline(color="red", linetype="dashed")+
  ggplot2::labs(x="Estimated Mean", y="True Mean")+
  ggplot2::theme_bw(base_size = 14) +
  ggplot2::theme(axis.title.y=ggplot2::element_text(colour="black"),
        axis.title.x = ggplot2::element_text(colour="black"),
        axis.text = ggplot2::element_text(colour = "black"),
        legend.position = c(0.8, 0.2),
        panel.border = ggplot2::element_blank(),
        panel.grid.major = ggplot2::element_blank(), 
        panel.grid.minor = ggplot2::element_blank(),
        axis.line = ggplot2::element_line(colour = "black"),
        plot.margin = ggplot2::margin(0, 0, 0, 0.2, "cm"))

```

